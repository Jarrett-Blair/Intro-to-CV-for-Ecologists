{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "DxkcnmfHw3xu",
   "metadata": {
    "id": "DxkcnmfHw3xu"
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2489cc",
   "metadata": {
    "id": "4e2489cc"
   },
   "source": [
    "### General data preparation\n",
    "The following splits the raw data set into training and testing data sets.\n",
    "\n",
    "After loading *carabids.csv* as `carabid`, the first step is to vizualize the species abundance distribution. This will help us decide a good abundance threshold for which species will be included in the first round of model training. Additional species can be added or removed based on insights gained in the model evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "carabid = read.csv(file.choose())\n",
    "\n",
    "sptable = table(carabid$SpeciesName)\n",
    "sptable = sort(sptable, decreasing = T)\n",
    "\n",
    "par(mar = c(2.5, 4, 2, 2))\n",
    "barplot(sptable, \n",
    "        ylab = \"Abundance\", \n",
    "        ylim = c(0,300), \n",
    "        names.arg = \"\"\n",
    "       )\n",
    "abline(h = 30, col = \"red\", lty = 2)\n",
    "mtext(\"Species\", side = 1, line = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4c22e",
   "metadata": {},
   "source": [
    "There appears to be an abundance dropoff at 30 specimens, so we will use that as our threshold. Any species below the threshold will be added to `spname`. \n",
    "\n",
    "We then subset `carabid` as `carabid_keep`, removing any species in `spname` and specimens with unknown species labels ('Carabidae sp.')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KHS1Nv3ABWcP",
   "metadata": {
    "id": "KHS1Nv3ABWcP"
   },
   "outputs": [],
   "source": [
    "'%!in%' = function(x,y)!('%in%'(x,y))\n",
    "\n",
    "spname = names(which(sptable < 30))\n",
    "\n",
    "carabid_keep = subset(carabid, SpeciesName %!in% c('Carabidae sp.',spname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02e4bc",
   "metadata": {
    "id": "fc02e4bc"
   },
   "source": [
    "Next, we will split the data into a training and testing set at a ratio of 80:20. Setting the seed here makes the code more reproduceable so the exact same split can be repeated again in the future if needed.\n",
    "\n",
    "When computing the sample size of the training dataset, we use `floor()` to round down to the nearest integer. Other rounding techniques can be used as well, what matters is that we get a whole integer value.\n",
    "\n",
    "We then take a random sample from our dataset and use it as our training dataset, with the remaining data being our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8e9e3",
   "metadata": {
    "id": "1ee8e9e3"
   },
   "outputs": [],
   "source": [
    "fractionTraining = 0.8\n",
    "fractionvalid = 0.2\n",
    "\n",
    "seed = 123\n",
    "set.seed(seed)\n",
    "\n",
    "# Compute sample sizes.\n",
    "sampleSizeTraining = floor(fractionTraining * nrow(carabid_keep))\n",
    "\n",
    "# Create the randomly-sampled indices for the dataframe. Use setdiff() to\n",
    "# avoid overlapping subsets of indices.\n",
    "indicesTraining = sort(sample(seq_len(nrow(carabid_keep)), size=sampleSizeTraining))\n",
    "indicestest = setdiff(seq_len(nrow(carabid_keep)), indicesTraining)\n",
    "\n",
    "dfTraining = carabid_keep[indicesTraining, ]\n",
    "dfTest = carabid_keep[indicestest, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6b696",
   "metadata": {
    "id": "b0b6b696"
   },
   "source": [
    "Now that we've split our data, we need to do some additional cleaning. First, we will remove unnecessary columns and confirm our predictor variables are numeric. Unnecessary columns will be any columns that do not contain class labels or numeric feature data that will be used to train the model. Our data contains some missing values, so a warning will appear stating that 'NAs introduced by coercion'. The algorithms we are training can handle NAs, so we can ignore that message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd32453",
   "metadata": {
    "id": "6dd32453"
   },
   "outputs": [],
   "source": [
    "#Remove unnecessary columns\n",
    "remove = c(1:17,25,29:32,43,47:50,55:57,65,73,81,89,97,105)\n",
    "dfTraining = dfTraining[,-remove]\n",
    "dfTest = dfTest[,-remove]\n",
    "\n",
    "#Make predictor variables numeric\n",
    "numcols = ncol(dfTraining)\n",
    "dfTraining[,2:numcols] = as.data.frame(lapply(dfTraining[,2:numcols], as.numeric))\n",
    "dfTest[,2:numcols] = as.data.frame(lapply(dfTest[,2:numcols], as.numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f7fdc",
   "metadata": {
    "id": "a27f7fdc"
   },
   "source": [
    "Finally, we will standardize our data using the `preProcess` function from the `caret` package. This prevents features with larger magnitudes from dominating the learning process and negatively impacting the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e60aa",
   "metadata": {
    "id": "427e60aa"
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "\n",
    "normParam = preProcess(dfTraining)\n",
    "norm.train = predict(normParam, dfTraining)\n",
    "norm.test = predict(normParam, dfTest)\n",
    "\n",
    "trainLabels = norm.train$SpeciesName\n",
    "\n",
    "testLabels = norm.test$SpeciesName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3jmpRpwGwx26",
   "metadata": {
    "id": "3jmpRpwGwx26"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1w9kmO8dFoh",
   "metadata": {
    "id": "u1w9kmO8dFoh"
   },
   "source": [
    "### XGBoost\n",
    "The following code will train an eXtreme Gradient Boosting (XGBoost) model.\n",
    "\n",
    "Before we begin, we must load the `xgboost` R package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DjmTDDaRdQpH",
   "metadata": {
    "id": "DjmTDDaRdQpH"
   },
   "outputs": [],
   "source": [
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O214OgIndU7R",
   "metadata": {
    "id": "O214OgIndU7R"
   },
   "source": [
    "The XGBoost model requires labels to be numeric starting at 0. The training data must also be reformatted as an `xgb.DMatrix` object to be compatible with the model. You can also format the test data as an `xgb.DMatrix` and include it as part of the `watchlist` object. This will allow you to monitor the performance of the model on the test data while the model trains, which will allow you to determine if/when the model becomes overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cRw8ibFidXlj",
   "metadata": {
    "id": "cRw8ibFidXlj"
   },
   "outputs": [],
   "source": [
    "trainlab = as.numeric(as.factor(trainLabels)) - 1\n",
    "testlab = as.numeric(as.factor(testLabels)) - 1\n",
    "\n",
    "dtrain <- xgb.DMatrix(label = trainlab, data = as.matrix(norm.train[,2:69]))\n",
    "dtest <- xgb.DMatrix(label = testlab, data = as.matrix(norm.test[,2:69]))\n",
    "watchlist = list(train = dtrain, test = dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I-MzebqQdbDo",
   "metadata": {
    "id": "I-MzebqQdbDo"
   },
   "source": [
    "Finally you can train your XGBoost model. A brief description of each of the model's parameters used here is as follows:\n",
    "\n",
    "`data`: Your `xgb.DMatrix` training data.\n",
    "\n",
    "`params`: A list of parameters. The parameters used here are:\n",
    "\n",
    "- `max.depth`: The maximum complexity of the model's trees. The higher this number is, the more variables each tree will consider, which allows the model to capture more complex interactions between features. However, setting the value too high can lead to overfitting.\n",
    "\n",
    "- `eta`: The learning rate of the model (min approaches 0, max = 1). A smaller eta value makes the boosting process more conservative by reducing the influence of each individual tree, preventing overfitting, but requiring more iterations (`nrounds`) to converge.\n",
    "\n",
    "- `num_class`: The number of 'classes' (i.e. species) in the model.\n",
    "\n",
    "- `eval.metric`: The evaluation performance metric (\"mlogloss\" [i.e. loss] or \"merror\" [i.e. 1 - accuracy]).\n",
    "\n",
    "- `objective`: The objective function. Setting this as \"multi:softprob\" will give us a classification probability matrix when we classify new data with this model. Using \"multi:softmax\" would only give us the top-1 classifications without probabilities.\n",
    "\n",
    "`watchlist`: Named datasets for model evaluation.\n",
    "\n",
    "`nrounds`: The number of boosting iterations in the model. Increase this as you decrease `eta`.\n",
    "\n",
    "`early_stopping_rounds`: Stops training if the performance (`eval.metric`) on a validation set does not improve for a specified number of consecutive rounds.\n",
    "\n",
    "`verbose`: The amount of information that will be printed as the model trains. 0 = no information, 1 = some evaluation metrics (`eval.metric`), 2 = more evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iq9N_8jfdgmz",
   "metadata": {
    "id": "iq9N_8jfdgmz"
   },
   "outputs": [],
   "source": [
    "num_class = 25\n",
    "params = list(max.depth = 9,\n",
    "              eta = 0.1,\n",
    "              num_class = num_class,\n",
    "              eval.metric = \"mlogloss\",\n",
    "              objective = \"multi:softprob\")\n",
    "model = xgb.train(data = dtrain,\n",
    "                  params = params,\n",
    "                  watchlist = watchlist,\n",
    "                  nrounds = 250,\n",
    "                  early_stopping_rounds = 25,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc265724",
   "metadata": {},
   "source": [
    "A learning curve can be plotting using the model's evaluation log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_train = model$evaluation_log$train_mlogloss\n",
    "logloss_test = model$evaluation_log$test_mlogloss\n",
    "\n",
    "# Plot the logloss learning curve\n",
    "plot(x = 1:length(logloss_train), \n",
    "     y = logloss_train, \n",
    "     type = \"l\", \n",
    "     col = \"blue\",\n",
    "     xlab = \"Number of Boosting Rounds\", \n",
    "     ylab = \"Logloss\",\n",
    "     main = \"Logloss Learning Curve\")\n",
    "lines(x = 1:length(logloss_test), \n",
    "      y = logloss_test, \n",
    "      type = \"l\", \n",
    "      col = \"red\")\n",
    "legend(\"topright\", \n",
    "       legend = c(\"Training\", \"Testing\"),\n",
    "       col = c(\"blue\", \"red\"), \n",
    "       lty = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yByZxiDSYnAR",
   "metadata": {
    "id": "yByZxiDSYnAR"
   },
   "source": [
    "You can then make classifications using your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YfCgB95cYv5f",
   "metadata": {
    "id": "YfCgB95cYv5f"
   },
   "outputs": [],
   "source": [
    "probs = predict(model, as.matrix(norm.test[,2:ncol(norm.test)]))\n",
    "probs = matrix(probs, nrow = num_class)\n",
    "probs = t(probs)\n",
    "probs = as.data.frame(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a47bb",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "\n",
    "The following code will train a Light Gradient Boosting Machine (LightGBM).\n",
    "\n",
    "To begin, we must load the `lightgbm` R package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c38a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd111a2",
   "metadata": {},
   "source": [
    "Like the XGBoost model above, the classification labels must be numeric and the data must be reformatted as an lgb.Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(as.matrix(norm.train[,-1]), label = as.numeric(as.factor(norm.train[,1])) - 1)\n",
    "dtest = lgb.Dataset(as.matrix(norm.test[,-1]), label = as.numeric(as.factor(norm.test[,1])) - 1)\n",
    "\n",
    "valids = list(train = dtrain,\n",
    "              test = dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3bfb8",
   "metadata": {},
   "source": [
    "The lightGBM model is now ready to be trained. A brief description of the model's parameters are as follows:\n",
    "\n",
    "`params`: A list of parameters. The parameters used here are:\n",
    "\n",
    "- `objective`: Specifies the loss function to be optimized during the training of the gradient boosting model (e.g. 'regression', 'binary', 'multiclass').\n",
    "\n",
    "- `metric`: Metrics to be evaluated on the evaluation set(s).\n",
    "\n",
    "- `num_class`: The number of 'classes' (i.e. species) in the model.\n",
    "\n",
    "- `learning_rate`: The step size learning rate of the model (same as `eta` in XGBoost).\n",
    "\n",
    "- `min_data_in_leaf`: The minimum data used in one leaf.\n",
    "\n",
    "- `num_leaves`: The maximum number of leaves allowed in a tree.\n",
    "\n",
    "- `max_depth`: Maximum depth of a LightGBM tree.\n",
    "    \n",
    "`data`: a lgb.Dataset object used for training.\n",
    "\n",
    "`nrounds`: number of training rounds.\n",
    "\n",
    "`valids`: a list of lgb.Dataset objects used for validation\n",
    "\n",
    "`verbose`: verbosity for output.\n",
    "\n",
    "`early_stopping_rounds`: Activates early stopping. When this parameter is non-null, training will stop if the evaluation of any metric on any validation set fails to improve for `early_stopping_rounds` consecutive boosting rounds. If training stops early, the returned model will have attribute `best_iter` set to the iteration number of the best iteration.\n",
    "\n",
    "For more information on LightGBM parameter tuning (particularly for the `num_leaves`, `min_data_in_leaf`, and `max_depth` parameters), see the LightGBM parameter tuning documenation https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 25\n",
    "params = list(objective = 'multiclass',\n",
    "              metric = 'multi_logloss',\n",
    "              num_class = num_class,\n",
    "              learning_rate = 0.1,\n",
    "              min_data_in_leaf = 75,\n",
    "              num_leaves = 40,\n",
    "              max_depth = 15)\n",
    "\n",
    "model = lgb.train(params = params,\n",
    "                  data = dtrain,\n",
    "                  nrounds = 100,\n",
    "                  valids = valids,\n",
    "                  verbose = -1,\n",
    "                  early_stopping_rounds = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c38872",
   "metadata": {},
   "source": [
    "A learning curve can be plotting using the model's evaluation log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss_train = lgb.get.eval.result(model, \"train\", \"multi_logloss\")\n",
    "logloss_test = lgb.get.eval.result(model, \"test\", \"multi_logloss\")\n",
    "\n",
    "# Plot the logloss learning curve\n",
    "plot(x = 1:length(logloss_train), \n",
    "     y = logloss_train, \n",
    "     type = \"l\", \n",
    "     col = \"blue\",\n",
    "     xlab = \"Number of Boosting Rounds\", \n",
    "     ylab = \"Logloss\",\n",
    "     main = \"Logloss Learning Curve\")\n",
    "lines(x = 1:length(logloss_test), \n",
    "      y = logloss_test, \n",
    "      type = \"l\", \n",
    "      col = \"red\")\n",
    "legend(\"topright\", \n",
    "       legend = c(\"Training\", \"Testing\"),\n",
    "       col = c(\"blue\", \"red\"), \n",
    "       lty = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d315540",
   "metadata": {},
   "source": [
    "You can then make classifications using your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ce3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(model, as.matrix(norm.test[,2:ncol(norm.test)]))\n",
    "probs = matrix(probs, nrow = num_class)\n",
    "probs = t(probs)\n",
    "probs = as.data.frame(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a7272",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "The following code will train a Random Forest classification model.\n",
    "\n",
    "To start, we must load the `randomForest` R package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ccd7b",
   "metadata": {},
   "source": [
    "Compared to the previous models, the training procedure for random forests is simple. The only data preparation step we need to do is ensure the class variable ('SpeciesName') is a factor. We can also use `na.roughfix` to fill any missing values in the test data. This allows predictions to be made on all the test specimens.\n",
    "\n",
    "A brief description of the model's parameters are as follows:\n",
    "`formula`: A formula describing the model to be fitted.\n",
    "\n",
    "`data`: A data frame containing the variables in the model.\n",
    "\n",
    "`ntree`: Number of trees to grow.\n",
    "\n",
    "`mtry`: Number of variables randomly sampled as candidates at each split. Higher mtry values can increase the model's capacity to capture intricate relationships in the data, but can also reduce the diversity of the trees in the random forest if mtry approaches the total number of features in the dataset. Conversely, a low mtry can prevent overfitting, but might reduce the model's overall predictive power.\n",
    "\n",
    "`na.action`: A function to specify the action to be taken if NAs are found.\n",
    "\n",
    "`importance`: Should the performance of the predictors be assessed?\n",
    "\n",
    "While random forest models are not inherently iterative, we can iterate over values of mtry using a training loop to find an optimal value. `mtry_range` determines the maximum range of mtry values that will be used. `early_stopping` determines when to stop iterating over different values of mtry based on the difference between the current mtry value and the best mtry. The model with the optimal value of mtry is saved as `best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MLmetrics)\n",
    "\n",
    "#Prepare data\n",
    "norm.train$SpeciesName = as.factor(norm.train$SpeciesName)\n",
    "norm.test$SpeciesName = as.factor(norm.test$SpeciesName)\n",
    "norm.test = na.roughfix(norm.test)\n",
    "\n",
    "#Initialize variables for the training loop. \n",
    "mtry_range = 5:50\n",
    "early_stopping = 5\n",
    "best_loss = Inf\n",
    "best_mtry = mtry_range[1]\n",
    "\n",
    "#Training loop\n",
    "for(mtry in mtry_range){\n",
    "  #Set seed for reproduceability\n",
    "  set.seed(123)\n",
    "    \n",
    "  #Train model\n",
    "  rf_model = randomForest(SpeciesName ~ ., \n",
    "                          data = norm.train, \n",
    "                          ntree = 500, \n",
    "                          mtry = mtry, \n",
    "                          na.action = na.roughfix, \n",
    "                          importance = TRUE)\n",
    "    \n",
    "  #Measure evaluation metrics\n",
    "  probs = predict(rf_model, norm.test, type = \"prob\")\n",
    "  colnames(probs) = levels(as.factor(testLabels))\n",
    "  preds = unlist(apply(probs, MARGIN = 1, FUN = function(x){names(which.max(x))}))\n",
    "  accuracy = mean(preds == norm.test$SpeciesName)\n",
    "  loss = MultiLogLoss(probs, testLabels)\n",
    "    \n",
    "  #Report evaluation metrics\n",
    "  print(paste(\"mtry =\", mtry, \"Accuracy =\", signif(accuracy, 5), \"Loss =\", signif(loss,5)))\n",
    "  \n",
    "  #If loss has been improved over previous best, save new values for the listed variables\n",
    "  if(loss < best_loss){\n",
    "    best_loss = loss\n",
    "    best_mtry = mtry\n",
    "    best_model = rf_model\n",
    "  }\n",
    "    \n",
    "  #If performance has not improved in [early_stopping] iterations, break the loop\n",
    "  if((mtry - best_mtry) >= early_stopping){\n",
    "    break\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20353469",
   "metadata": {},
   "source": [
    "You can then make classifications using your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46be4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict(best_model, norm.test, type = \"prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wyIIL9fIfrxN",
   "metadata": {
    "id": "wyIIL9fIfrxN"
   },
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OgoQfld4Y_fx",
   "metadata": {
    "id": "OgoQfld4Y_fx"
   },
   "source": [
    "### Basic metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oxNQRLOaLkXi",
   "metadata": {
    "id": "oxNQRLOaLkXi"
   },
   "source": [
    "Most basic performance metrics for your model can be easily measured using the `confusionMatrix` function from the `caret` package.\n",
    "\n",
    "To make our data compatible with the `confusionMatrix` function, we will need to convert our probability matrix to a vector of predicted classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2SzIgd2YZAB5",
   "metadata": {
    "id": "2SzIgd2YZAB5"
   },
   "outputs": [],
   "source": [
    "library(caret)\n",
    "colnames(probs) = levels(as.factor(testLabels))\n",
    "preds = unlist(apply(probs, MARGIN = 1, FUN = function(x){names(which.max(x))}))\n",
    "confmat = confusionMatrix(as.factor(preds), as.factor(testLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de0575",
   "metadata": {},
   "source": [
    "Measuring loss can easily be done using the `MLmetrics` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a617b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MLmetrics)\n",
    "loss = MultiLogLoss(probs, testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l7Bf7y_cv4dV",
   "metadata": {
    "id": "l7Bf7y_cv4dV"
   },
   "source": [
    "Accuracy and related metrics can be found in `confmat$overall`.\n",
    "\n",
    "Many useful metrics for comparing performance within classes can be found in `confmat$byclass`, such as precision, recall, and F1 score. You can also measure the macro average of these metrics by taking measuring the average across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FwCRABAfv6E_",
   "metadata": {
    "id": "FwCRABAfv6E_"
   },
   "outputs": [],
   "source": [
    "accuracy = confmat$overall[1]\n",
    "#Simply using mean() to measure macro averages can work, but might\n",
    "#give an incorrect result if values are missing\n",
    "f1 = sum(confmat$byClass[,\"F1\"], na.rm = T)/nrow(confmat$byClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6f57e",
   "metadata": {},
   "source": [
    "The relationship between classification performance and species prevalence in the data can be visualized using data in `confmat$byclass`. Please note this assumes the relative species prevalence is the same in the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prevalence = confmat$byClass[,\"Prevalence\"]\n",
    "F1 = confmat$byClass[,\"F1\"]\n",
    "\n",
    "f1_lm = lm(F1~Prevalence)\n",
    "plot(F1~Prevalence)\n",
    "abline(f1_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7dKmnSv8SG",
   "metadata": {
    "id": "2e7dKmnSv8SG"
   },
   "source": [
    "Our `confmat` object also contains a confusion matrix that we can plot using `ggplot2`. \n",
    "\n",
    "First, the confusion matrix must be reformated using the `melt` function from `reshape2`.\n",
    "\n",
    "Then, we transform the values in the confusion matrix to be the proportion of the total number of true observations for each species. We do this using a custom `prop` function and functions within the `dplyr` package. This will allow our plot to be shaded proportionally for each species.\n",
    "\n",
    "The resulting plot can be plotted normally, or saved as an interactive .html file using `plotly` & `htmlwidgets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfBl1gm7v-Ov",
   "metadata": {
    "id": "KfBl1gm7v-Ov"
   },
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(ggplot2)\n",
    "library(plotly)\n",
    "library(htmlwidgets)\n",
    "\n",
    "conf_df = as.data.frame(confmat$table)\n",
    "conf_df = melt(conf_df)\n",
    "\n",
    "prop = function(x){\n",
    "  x/sum(x)\n",
    "}\n",
    "\n",
    "conf_df = conf_df %>%\n",
    "  group_by(Reference) %>%\n",
    "  mutate(prop = prop(value))\n",
    "\n",
    "#Creating an ordered vectors of shortened species names\n",
    "short_names = ordered(levels(factor(conf_df$Reference)))\n",
    "short_names = str_replace(short_names, \"(\\\\w)\\\\w+\\\\s(\\\\w+)(\\\\s\\\\w+)?\", \"\\\\1. \\\\2\")\n",
    "\n",
    "gg = ggplot(conf_df, aes(x = Prediction, y = Reference, fill = prop)) +\n",
    "  geom_tile() +\n",
    "  scale_fill_gradient(low = \"#201547\", high = \"#00BCE1\", name = \"Proportion\") +\n",
    "  scale_x_discrete(labels = short_names, position = \"top\") +\n",
    "  labs(y = \"Actual\") +\n",
    "  annotate(\"text\", x = 12.5, y = -0.5, label = \"Predicted\", hjust = 0.5, vjust = 0) +\n",
    "  theme(axis.text.x = element_text(angle = 30, hjust = 0, vjust = 1),\n",
    "        axis.title.x = element_blank(),\n",
    "        panel.grid = element_blank(),\n",
    "        panel.background = element_rect(fill = \"white\"))\n",
    "\n",
    "p = ggplotly(gg)\n",
    "p = p %>% layout(xaxis = list(side = \"top\"))\n",
    "\n",
    "saveWidget(p, \"confmat.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7UDF3a8sv_7H",
   "metadata": {
    "id": "7UDF3a8sv_7H"
   },
   "source": [
    "### Top x accuracy\n",
    "\n",
    "If you want to measure your model's accuracy if it had several attempts to make a classification, you can use these custom functions. `topxacc` will return your models accuracy when given 'x' number of attempts, while `topxpreds` returns the top 'x' most likely classifications for your test specimens. (e.g. top 3 accuracy or top 3 predictions).\n",
    "\n",
    "These functions work by finding the top 'x' highest probability classes for any given classification in your classification probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wNXeMAQ9wBwi",
   "metadata": {
    "id": "wNXeMAQ9wBwi"
   },
   "outputs": [],
   "source": [
    "topxacc = function(x, testlab, prob){\n",
    "  #Creating a dataframe of top 'x' predictions\n",
    "  topxpreds = data.frame(matrix(NA, nrow = nrow(prob), ncol = x))\n",
    "  for(i in 1:nrow(prob)){\n",
    "    #Each row is reordered by descending probability. The name of the top 'x' columns\n",
    "    #from each row are recorded in topxpreds\n",
    "    topxpreds[i,] =  names(prob[i,order(as.numeric(prob[i,]), decreasing = T)])[1:x]\n",
    "  }\n",
    "  topxaccuracy = sum(testlab == topxpreds)/length(testlab)\n",
    "  \n",
    "  return(topxaccuracy)\n",
    "}\n",
    "\n",
    "topxpreds = function(x, testlab, prob){\n",
    "  #Creating a dataframe of top 'x' predictions\n",
    "  topxpreds = data.frame(matrix(NA, nrow = nrow(prob), ncol = x))\n",
    "  for(i in 1:nrow(prob)){\n",
    "    #Each row is reordered by descending probability. The name of the top 'x' columns\n",
    "    #from each row are recorded in topxpreds\n",
    "    topxpreds[i,] =  names(prob[i,order(as.numeric(prob[i,]), decreasing = T)])[1:x]\n",
    "  }\n",
    "  \n",
    "  return(topxpreds)\n",
    "}\n",
    "\n",
    "top3acc = topxacc(3, testLabels, probs)\n",
    "top3preds = topxpreds(3, testLabels, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93740859",
   "metadata": {},
   "source": [
    "### ROC & AUC\n",
    "Receiver Operating Characteristic (ROC) curves show the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity) for varying classification thresholds. ROC curves are typically used in binary classification problems, but can be adapted to multi-class classification problems using a one vs. rest approach, where only one class is treated as the positive class and the remaining are treated as negative classes. This is repeated for each class in the dataset, resulting in an ROC curve for each class.\n",
    "\n",
    "\n",
    "The AUC (Area Under the Curve) of an ROC curve quantifies the overall performance of a binary classification model by measuring the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance according to the model's predicted probabilities. A higher AUC value indicates better discrimination and predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3584b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 25\n",
    "testlab = as.numeric(as.factor(testLabels)) - 1\n",
    "\n",
    "roc_objs = lapply(1:num_classes, function(class_index) {\n",
    "  roc_obj = roc(testlab == class_index, probs[, class_index])\n",
    "  roc_obj\n",
    "})\n",
    "auc_values = sapply(roc_objs, function(roc_obj) auc(roc_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11244",
   "metadata": {},
   "source": [
    "To print any individual ROC curve, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c3c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace i with whichever class index you want to plot an ROC curve for\n",
    "i = 1\n",
    "plot(roc_objs[[i]], main = paste(\"ROC Curve - Class\", i), print.auc = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V84hHnxuwFO0",
   "metadata": {
    "id": "V84hHnxuwFO0"
   },
   "source": [
    "### Hierarchical classification\n",
    "\n",
    "Hierarchical classifiers allow you to make classifications at multiple taxonomic levels simultaneously. To set up a hierarchical classifier, you will need to make a taxonomic hierarchy dataframe for your data. First, create a vector called `hierarchylevels` that contains the column names in your dataset of the taxonomic levels you want to include. Then, use this custom `hierarchy` function to create your taxonomic hierarchy dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fEkxB493wH1N",
   "metadata": {
    "id": "fEkxB493wH1N"
   },
   "outputs": [],
   "source": [
    "hierarchylevels = c(\"SpeciesName\", \n",
    "                    \"Genus\", \n",
    "                    \"Subtribe\", \n",
    "                    \"Tribe\", \n",
    "                    \"Supertribe\", \n",
    "                    \"Subfamily\")\n",
    "\n",
    "hierarchy = function(data, ranks){\n",
    "  \n",
    "  baselabels = data[,ranks[1]]\n",
    "  \n",
    "  #Get all unique base labels\n",
    "  uniquelabels = levels(as.factor(baselabels))\n",
    "  \n",
    "  #Create hierarchy DF\n",
    "  hierarchy = data.frame(matrix(NA, nrow = length(uniquelabels), ncol = length(ranks)))\n",
    "  colnames(hierarchy) = ranks\n",
    "  #Set first column of DF as the unique baselabels\n",
    "  hierarchy[,1] = uniquelabels\n",
    "  #For loop starting with second taxonomic level/column\n",
    "  for(i in 2:ncol(hierarchy)){\n",
    "    #Iterate through each unique LITL label\n",
    "    for(j in 1:length(uniquelabels)){\n",
    "      row = which(baselabels == uniquelabels[j])[1]\n",
    "      hierarchy[j,i] = as.character(data[row,ranks[i]])\n",
    "    }\n",
    "  }\n",
    "  return(hierarchy)\n",
    "}\n",
    "\n",
    "myhierarchy = hierarchy(carabid, hierarchylevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "youfjvxTwL7o",
   "metadata": {
    "id": "youfjvxTwL7o"
   },
   "source": [
    "Once you have your hierarchy dataframe, you can use it as a reference to convert your model's classifications into hierarchical classifications using this custom `hpredict` function. You can also use `hpredict` on your test labels for easier comparison with with your hierarchical classifications. You can then measure performance metrics at any taxonomic level using `confusionMatrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v6xPAmEVwPHh",
   "metadata": {
    "id": "v6xPAmEVwPHh"
   },
   "outputs": [],
   "source": [
    "preds = top3preds[,1]\n",
    "\n",
    "hpredict = function(preds, hierarchy){\n",
    "  #Initializing prediction dataframe\n",
    "  preddf = data.frame(matrix(NA, nrow = length(preds), ncol = ncol(hierarchy)))\n",
    "  colnames(preddf) = colnames(hierarchy)\n",
    "  preddf[,1] = preds\n",
    "  \n",
    "  \n",
    "  for(i in 2:ncol(hierarchy)){\n",
    "    for(j in 1:nrow(preddf)){\n",
    "      #For the current pred (in this case preddf[j,i-1]), find the first match\n",
    "      #in the hierarchy dataframe and assign the label in the next column up\n",
    "      #as preddf[j,i].\n",
    "      preddf[j,i] = hierarchy[which(hierarchy[,i-1] == preddf[j,i-1])[1], i]\n",
    "    }\n",
    "  }  \n",
    "  return(preddf)\n",
    "}\n",
    "\n",
    "hpreds = hpredict(preds, myhierarchy)\n",
    "htest = hpredict(testLabels, myhierarchy)\n",
    "\n",
    "genusconfmat = confusionMatrix(as.factor(hpreds$Genus), as.factor(htest$Genus))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "138bcbb7"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
